{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a4af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'python (Python)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/python/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resume NLP Processing with spaCy, TF-IDF, Word2Vec, and BERT\n",
    "=============================================================\n",
    "This module handles the NLP processing of resumes, including:\n",
    "1. Tokenization and Lemmatization using spaCy\n",
    "2. Vectorization using TF-IDF, Word2Vec, and BERT\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # Fixed TF-IDF import\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "\n",
    "# Load spaCy model - you can choose different sizes based on your needs\n",
    "# 'en_core_web_sm' is smaller/faster, 'en_core_web_lg' has word vectors included\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class ResumeNLPProcessor:\n",
    "    \"\"\"Class for processing resumes with NLP techniques\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the NLP processor\"\"\"\n",
    "        self.nlp = nlp\n",
    "        # Load BERT tokenizer and model\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # Set model to evaluation mode\n",
    "        self.bert_model.eval()\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean text by removing special characters and extra whitespace\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text from resume\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower().strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process text with spaCy to get tokens and lemmas\n",
    "        \n",
    "        Args:\n",
    "            text: Cleaned text from resume\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with tokens and lemmas\n",
    "        \"\"\"\n",
    "        # Process with spaCy\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Extract tokens (excluding stopwords and punctuation)\n",
    "        tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "        \n",
    "        # Extract lemmas (normalized word forms)\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        \n",
    "        # Get named entities (might be useful for extracting skills, education, etc.)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'lemmas': lemmas,\n",
    "            'entities': entities,\n",
    "            'processed_text': ' '.join(lemmas)\n",
    "        }\n",
    "    \n",
    "    def vectorize_tfidf(self, documents: List[str]) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Convert documents to TF-IDF vectors\n",
    "        \n",
    "        Args:\n",
    "            documents: List of processed texts\n",
    "            \n",
    "        Returns:\n",
    "            TF-IDF matrix and feature names\n",
    "        \"\"\"\n",
    "        # Initialize the TF-IDF vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "        \n",
    "        # Fit and transform the documents\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        \n",
    "        return tfidf_matrix, feature_names\n",
    "    \n",
    "    def train_word2vec(self, tokenized_documents: List[List[str]], vector_size: int = 100) -> Word2Vec:\n",
    "        \"\"\"\n",
    "        Train a Word2Vec model on the tokenized documents\n",
    "        \n",
    "        Args:\n",
    "            tokenized_documents: List of lists of tokens\n",
    "            vector_size: Dimensionality of vectors\n",
    "            \n",
    "        Returns:\n",
    "            Trained Word2Vec model\n",
    "        \"\"\"\n",
    "        # Train Word2Vec model\n",
    "        model = Word2Vec(\n",
    "            sentences=tokenized_documents,\n",
    "            vector_size=vector_size,\n",
    "            window=5,\n",
    "            min_count=1,\n",
    "            workers=4\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_doc_vectors_word2vec(self, tokenized_documents: List[List[str]], \n",
    "                                word2vec_model: Word2Vec) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create document vectors by averaging Word2Vec word vectors\n",
    "        \n",
    "        Args:\n",
    "            tokenized_documents: List of lists of tokens\n",
    "            word2vec_model: Trained Word2Vec model\n",
    "            \n",
    "        Returns:\n",
    "            List of document vectors\n",
    "        \"\"\"\n",
    "        doc_vectors = []\n",
    "        \n",
    "        for doc in tokenized_documents:\n",
    "            # Filter tokens that are in the model vocabulary\n",
    "            valid_tokens = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "            \n",
    "            if len(valid_tokens) > 0:\n",
    "                # Average the word vectors\n",
    "                doc_vector = np.mean([word2vec_model.wv[word] for word in valid_tokens], axis=0)\n",
    "            else:\n",
    "                # If no valid tokens, use zero vector\n",
    "                doc_vector = np.zeros(word2vec_model.vector_size)\n",
    "                \n",
    "            doc_vectors.append(doc_vector)\n",
    "            \n",
    "        return doc_vectors\n",
    "    \n",
    "    def get_bert_embeddings(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get BERT embeddings for a text\n",
    "        \n",
    "        Args:\n",
    "            text: Text to encode\n",
    "            \n",
    "        Returns:\n",
    "            BERT embedding vector\n",
    "        \"\"\"\n",
    "        # Tokenize and prepare for BERT\n",
    "        encoded_input = self.bert_tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get BERT embedding (without gradient calculation)\n",
    "        with torch.no_grad():\n",
    "            output = self.bert_model(**encoded_input)\n",
    "        \n",
    "        # Use the [CLS] token embedding as the document embedding\n",
    "        # (This is a common approach for document-level embeddings)\n",
    "        return output.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "    \n",
    "    def get_doc_vectors_bert(self, documents: List[str]) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create document vectors using BERT embeddings\n",
    "        \n",
    "        Args:\n",
    "            documents: List of texts\n",
    "            \n",
    "        Returns:\n",
    "            List of document vectors\n",
    "        \"\"\"\n",
    "        return [self.get_bert_embeddings(doc) for doc in documents]\n",
    "    \n",
    "    def compute_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two vectors\n",
    "        \n",
    "        Args:\n",
    "            vec1: First vector\n",
    "            vec2: Second vector\n",
    "            \n",
    "        Returns:\n",
    "            Cosine similarity score\n",
    "        \"\"\"\n",
    "        return cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0]\n",
    "    \n",
    "    def process_resume_batch(self, resume_texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a batch of resumes and return processed data\n",
    "        \n",
    "        Args:\n",
    "            resume_texts: List of raw resume texts\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processed data\n",
    "        \"\"\"\n",
    "        # Clean texts\n",
    "        cleaned_texts = [self.clean_text(text) for text in resume_texts]\n",
    "        \n",
    "        # Tokenize and lemmatize\n",
    "        processed_data = [self.tokenize_and_lemmatize(text) for text in cleaned_texts]\n",
    "        \n",
    "        # Extract processed texts and tokens\n",
    "        processed_texts = [data['processed_text'] for data in processed_data]\n",
    "        tokenized_texts = [data['tokens'] for data in processed_data]\n",
    "        \n",
    "        # Generate TF-IDF vectors\n",
    "        tfidf_matrix, feature_names = self.vectorize_tfidf(processed_texts)\n",
    "        \n",
    "        # Train Word2Vec model\n",
    "        w2v_model = self.train_word2vec(tokenized_texts)\n",
    "        \n",
    "        # Get document vectors using Word2Vec\n",
    "        w2v_vectors = self.get_doc_vectors_word2vec(tokenized_texts, w2v_model)\n",
    "        \n",
    "        # Get BERT embeddings (this can be computationally expensive)\n",
    "        # We'll process only a few documents for demonstration\n",
    "        bert_vectors = self.get_doc_vectors_bert(processed_texts[:3])\n",
    "        \n",
    "        return {\n",
    "            'processed_data': processed_data,\n",
    "            'tfidf_matrix': tfidf_matrix,\n",
    "            'tfidf_features': feature_names,\n",
    "            'word2vec_model': w2v_model,\n",
    "            'word2vec_vectors': w2v_vectors,\n",
    "            'bert_vectors': bert_vectors\n",
    "        }\n",
    "\n",
    "    def visualize_vector_similarities(self, vectors: List[np.ndarray], labels: List[str] = None):\n",
    "        \"\"\"\n",
    "        Visualize similarities between vectors\n",
    "        \n",
    "        Args:\n",
    "            vectors: List of document vectors\n",
    "            labels: Labels for the documents\n",
    "        \"\"\"\n",
    "        n = len(vectors)\n",
    "        similarity_matrix = np.zeros((n, n))\n",
    "        \n",
    "        # Compute pairwise similarities\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                similarity_matrix[i, j] = self.compute_similarity(vectors[i], vectors[j])\n",
    "        \n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(similarity_matrix, annot=True, cmap='Blues',\n",
    "                   xticklabels=labels if labels else [f\"Doc {i}\" for i in range(n)],\n",
    "                   yticklabels=labels if labels else [f\"Doc {i}\" for i in range(n)])\n",
    "        plt.title('Document Similarity Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def compare_job_to_resumes(self, job_description: str, resume_texts: List[str], \n",
    "                              method: str = 'tfidf') -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Compare job description to resumes and return similarity scores\n",
    "        \n",
    "        Args:\n",
    "            job_description: Job description text\n",
    "            resume_texts: List of resume texts\n",
    "            method: Vectorization method ('tfidf', 'word2vec', or 'bert')\n",
    "            \n",
    "        Returns:\n",
    "            List of (resume_index, similarity_score) tuples sorted by similarity\n",
    "        \"\"\"\n",
    "        # Clean and process job description\n",
    "        clean_job = self.clean_text(job_description)\n",
    "        job_processed = self.tokenize_and_lemmatize(clean_job)\n",
    "        job_text = job_processed['processed_text']\n",
    "        \n",
    "        # Clean and process resumes\n",
    "        clean_resumes = [self.clean_text(text) for text in resume_texts]\n",
    "        resume_processed = [self.tokenize_and_lemmatize(text) for text in clean_resumes]\n",
    "        resume_texts_processed = [data['processed_text'] for data in resume_processed]\n",
    "        \n",
    "        # Calculate similarity based on method\n",
    "        if method == 'tfidf':\n",
    "            # Combine job and resumes for TF-IDF\n",
    "            all_texts = [job_text] + resume_texts_processed\n",
    "            tfidf_matrix, _ = self.vectorize_tfidf(all_texts)\n",
    "            \n",
    "            # Extract job vector and resume vectors\n",
    "            job_vector = tfidf_matrix[0]\n",
    "            resume_vectors = tfidf_matrix[1:]\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = [\n",
    "                (i, cosine_similarity(job_vector, rv.reshape(1, -1))[0][0])\n",
    "                for i, rv in enumerate(resume_vectors)\n",
    "            ]\n",
    "            \n",
    "        elif method == 'word2vec':\n",
    "            # Tokenize all documents\n",
    "            job_tokens = job_processed['tokens']\n",
    "            resume_tokens = [data['tokens'] for data in resume_processed]\n",
    "            \n",
    "            # Train Word2Vec on all documents\n",
    "            all_tokens = [job_tokens] + resume_tokens\n",
    "            w2v_model = self.train_word2vec(all_tokens)\n",
    "            \n",
    "            # Get document vectors\n",
    "            all_vectors = self.get_doc_vectors_word2vec(all_tokens, w2v_model)\n",
    "            job_vector = all_vectors[0]\n",
    "            resume_vectors = all_vectors[1:]\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = [\n",
    "                (i, self.compute_similarity(job_vector, rv))\n",
    "                for i, rv in enumerate(resume_vectors)\n",
    "            ]\n",
    "            \n",
    "        elif method == 'bert':\n",
    "            # Get BERT embeddings\n",
    "            job_vector = self.get_bert_embeddings(job_text)\n",
    "            resume_vectors = self.get_doc_vectors_bert(resume_texts_processed)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = [\n",
    "                (i, self.compute_similarity(job_vector, rv))\n",
    "                for i, rv in enumerate(resume_vectors)\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        # Sort by similarity score (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return similarities\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data (in a real scenario, these would come from your database)\n",
    "    sample_resumes = [\n",
    "        \"\"\"\n",
    "        John Doe\n",
    "        Software Engineer with 5 years of experience in Python and Machine Learning.\n",
    "        Skills: Python, TensorFlow, PyTorch, SQL, Docker\n",
    "        Experience: Senior Developer at Tech Co (2018-2023)\n",
    "        Education: MS Computer Science, Stanford University\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        Jane Smith\n",
    "        Data Scientist specializing in NLP and Deep Learning.\n",
    "        Skills: Python, R, BERT, Word2Vec, spaCy, Keras\n",
    "        Experience: Data Scientist at AI Labs (2019-2023)\n",
    "        Education: PhD in Computational Linguistics, MIT\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        Bob Johnson\n",
    "        Frontend Developer with UI/UX expertise.\n",
    "        Skills: JavaScript, React, HTML, CSS, Figma, Adobe XD\n",
    "        Experience: UI Developer at Design Agency (2017-2023)\n",
    "        Education: BFA Digital Design, Rhode Island School of Design\n",
    "        \"\"\"\n",
    "    ]\n",
    "    \n",
    "    sample_job = \"\"\"\n",
    "    We are looking for a Data Scientist with NLP expertise.\n",
    "    Required skills: Python, Machine Learning, Deep Learning, NLP, BERT\n",
    "    Responsibilities: Build text classification models, develop information extraction systems\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = ResumeNLPProcessor()\n",
    "    \n",
    "    # Process resumes\n",
    "    results = processor.process_resume_batch(sample_resumes)\n",
    "    \n",
    "    # Compare job to resumes\n",
    "    print(\"TF-IDF Similarity Ranking:\")\n",
    "    tfidf_rankings = processor.compare_job_to_resumes(sample_job, sample_resumes, method='tfidf')\n",
    "    for idx, score in tfidf_rankings:\n",
    "        print(f\"Resume {idx}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\nWord2Vec Similarity Ranking:\")\n",
    "    w2v_rankings = processor.compare_job_to_resumes(sample_job, sample_resumes, method='word2vec')\n",
    "    for idx, score in w2v_rankings:\n",
    "        print(f\"Resume {idx}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\nBERT Similarity Ranking:\")\n",
    "    bert_rankings = processor.compare_job_to_resumes(sample_job, sample_resumes, method='bert')\n",
    "    for idx, score in bert_rankings:\n",
    "        print(f\"Resume {idx}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
